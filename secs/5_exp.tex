\section{Experiment}
\label{sec:experiments}

\subsection{Experiment Setup}
\label{sec:exp_setup}

% -----------------------------
% (Moved from: Evaluation Protocol)
% -----------------------------
\paragraph{Evaluation protocol.}
To characterize the robustness of the learned local planner under stochastic disturbances, we evaluate our method and baselines on a standardized benchmark suite.
Each method is evaluated on \textbf{200} randomly generated environments with a fixed start and goal placed at two opposite corners of the map to ensure comparable path length budgets across runs. The environment features a $50 \times 50$ map size with randomized obstacle layouts and flow fields.

\paragraph{Environment instantiation.}
Each episode is generated by sampling: (i) flow sources (e.g., vortices) with random locations, orientations, and strengths; (ii) static obstacles with random positions and sizes; and (iii) dynamic obstacles whose initial states and motion patterns are sampled from the distribution described in Sec.~\ref{sec:method_env}.
Dynamic obstacles spawn within specific regions and move with velocity profiles influenced by the local current, requiring the agent to anticipate their drift.
A square boundary is enforced in each environment; leaving the valid region is counted as an \emph{out-of-bounds} failure.

\paragraph{Control and rollout settings.}
The control time step is set to $\Delta t=1.0$\,s.
An episode terminates when the agent reaches the goal, collides with an obstacle, runs out of the maximum step budget (1000 steps), or violates the boundary constraint.

\paragraph{Metrics.}
We report four metrics following common practice in local navigation benchmarks:
\begin{itemize}
    \item \textbf{Success rate:} fraction of episodes that reach the goal without collision or out-of-bounds.
    \item \textbf{Out-of-bounds rate:} fraction of episodes ending due to boundary violation.
    \item \textbf{Average time:} mean time-to-goal computed \emph{only} over successful episodes.
    \item \textbf{Average energy:} an action-effort proxy defined as the sum of the magnitudes of control commands along the trajectory, computed over successful episodes.
\end{itemize}

% -----------------------------
% (Moved from: Risk-Sensitive IQN Variants)
% -----------------------------
\paragraph{Risk-sensitive IQN variants.}
We evaluate our Distributional-Risk Planner instantiated with \textbf{Implicit Quantile Networks (IQN)} under both \textbf{fixed} and \textbf{adaptive} risk sensitivity.
Concretely, we adopt a CVaR-style distortion with threshold $\phi \in (0,1]$ and test fixed settings $\phi \in \{0.25,0.50,0.75,1.0\}$ to span a spectrum from strongly tail-focused (conservative) to risk-neutral (greedy).

In addition, we implement a \textbf{distance-adaptive} threshold that increases risk sensitivity as the agent approaches obstacles:
\begin{equation}
\phi =
\begin{cases}
\min(d(\mathbf{X},\mathbf{X}_O))/d_0, & \text{if } \min(d(\mathbf{X},\mathbf{X}_O)) \le d_0,\\
1.0, & \text{otherwise,}
\end{cases}
\label{eq:adaptive_phi}
\end{equation}
where $\mathbf{X}$ is the agent position, $\mathbf{X}_O$ denotes the set of obstacle positions (including dynamic obstacles), and $d_0$ is the sensing range. This design yields near-greedy behavior in free space while automatically emphasizing tail-risk when the environment becomes cluttered.

% -----------------------------
% (Moved from: Baselines)
% -----------------------------
\paragraph{Baselines.}
We compare against both \textbf{classical reactive planners} and \textbf{value-based DRL} baselines.

\paragraph{Classical planners.}
We include (i) \textbf{Artificial Potential Fields (APF)}, which combines an attractive potential toward the goal with a repulsive term induced by obstacle proximity, and (ii) a \textbf{Bug Algorithm (BA)} variant in the spirit of VisBug, which alternates between goal-seeking and boundary-following when an obstacle blocks the direct path.

\paragraph{Deep RL baselines (expected return).}
Beyond the standard \textbf{DQN} baseline used in prior work, we evaluate \textbf{Greedy IQN} (IQN with $\phi=1.0$) to isolate the effect of the adaptive risk mechanism.
All DRL baselines share the same observation encoding and action space as our method, and are trained under the identical curriculum schedule, replay buffer configuration, and exploration strategy.

% =====================================================================
\subsection{Results}
\label{sec:results}

\paragraph{Overall performance.}
Table~\ref{tab:main_results} summarizes the evaluation results over 200 random episodes. The proposed \textbf{Adaptive IQN} planner achieves the highest success rate of \textbf{96.50\%}, outperforming the risk-neutral greedy variant (94.00\%) and the standard expected-return DQN baseline (91.50\%).
This performance advantage highlights the robustness of distributional RL in handling the aleatoric uncertainty introduced by unknown currents and dynamic obstacles.
While classical planners like APF and BA show lower calculation times, their success rates are significantly compromised in this dynamic setting (13.50\% and 67.50\%, respectively), primarily due to local minima and inability to anticipate flow-induced drift.

\paragraph{Energy efficiency.}
A key contribution of our work is the integration of energy-aware constraints. The Adaptive IQN agent not only navigates safely but also demonstrates superior energy efficiency, consuming an average of \textbf{34.62 J} per successful episode.
This represents a \textbf{3.96\%} energy saving compared to the Greedy IQN baseline (36.04 J) and a substantial \textbf{15\%} reduction compared to DQN (40.78 J).
This improvement suggests that the adaptive risk mechanism encourages smoother, more conservative maneuvers that avoid costly high-energy corrections often required to recover from near-collision states.

\paragraph{Effect of adaptive risk sensitivity.}
The results validate the efficacy of the distance-adaptive risk threshold $\phi$. By modulating risk sensitivity based on obstacle proximity, the Adaptive IQN planner effectively balances safety and efficiency.
In open waters, it behaves similarly to the greedy policy, maintaining competitive travel times (26.20 s vs 25.96 s for Greedy IQN). However, near obstacles, the shift to a risk-averse policy prevents catastrophic collisions that expected-return methods might underestimate.
Qualitatively, this is observed as the agent taking earlier, smaller distinct actions to steer clear of potential hazards, rather than reacting reactively at the last moment.

\paragraph{Runtime analysis.}
Table~\ref{tab:runtime} presents the computational cost.
Classical methods (APF, BA) are extremely lightweight ($\sim 0.03-0.04$ ms), but their lack of lookahead capability severely limits their robustness.
Among learning-based methods, the distributional planners (Adaptive IQN: 0.43 ms, Greedy IQN: 0.38 ms) incur a slight computational overhead compared to DQN (0.34 ms) due to the quantile embedding and aggregation steps. Nevertheless, this inference time is well within real-time control requirements (typically $>100$ ms budgets) for USV navigation, making the proposed approach feasible for onboard deployment.

\paragraph{Classical planners under flow disturbances.}
The low success rate of APF (13.50\%) illustrates the severity of the "local minima" problem in flow-disturbed environments. Without global guidance or a learned policy, APF agents frequently get trapped in oscillatory states where the attractive goal force is exactly counteracted by repulsive obstacle forces and flow disturbances.
Bug Algorithms (BA) fare better (67.50\%) by following boundaries, but often result in suboptimal, energy-intensive paths (43.53 J) due to reactive "wall-following" behaviors that fight against the current rather than exploiting it.

\paragraph{Conclusion of experiments.}
In summary, the experiments demonstrate that the Adaptive IQN planner successfully bridges the gap between safety and efficiency. It significantly outperforms baselines in reliability (Success Rate) while simultaneously reducing energy consumption, validating the hypothesis that risk-aware distributional RL is a superior framework for navigation in complex, dynamic marine environments.

% -----------------------------
% Tables (placeholders)
% -----------------------------
\begin{table}[t]
\centering
\caption{Main evaluation results on 200 random episodes. We report Success Rate, Average Time, Average Energy, and Average Path Length for successful runs.}
\label{tab:main_results}
\begin{tabular}{lcccc}
\hline
Agent & Success $\uparrow$ & Time(s) $\downarrow$ & Energy (J) $\downarrow$ & Path Length (m) \\
\hline
\textbf{Adaptive IQN (Ours)} & \textbf{96.50\%} & 26.20 & \textbf{34.62} & 48.74 \\
Greedy IQN ($\phi=1.0$) & 94.00\% & \textbf{25.96} & 36.04 & 48.51 \\
DQN  & 91.50\% & 26.11 & 40.78 & 49.37 \\
APF  & 13.50\% & 21.63 & 29.52 & \textbf{40.71} \\
BA   & 67.50\% & 30.28 & 43.53 & 48.25 \\
\hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Runtime per action decision (Average per step).}
\label{tab:runtime}
\begin{tabular}{lc}
\hline
Agent & Avg Calculation Time (ms) $\downarrow$ \\
\hline
Adaptive IQN (Ours) & 0.43 \\
Greedy IQN ($\phi=1.0$) & 0.38 \\
DQN  & 0.34 \\
APF  & \textbf{0.04} \\
BA   & \textbf{0.03} \\
\hline
\end{tabular}
\end{table}