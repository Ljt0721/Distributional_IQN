\section{Experiment}
\label{sec:experiments}

\subsection{Experiment Setup}
\label{sec:exp_setup}

% -----------------------------
% (Moved from: Evaluation Protocol)
% -----------------------------
\paragraph{Evaluation protocol.}
To characterize the robustness of the learned local planner under stochastic disturbances, we follow a two-tier evaluation protocol adapted from prior distributional-RL navigation studies.
We construct two test suites with different environment difficulty levels, denoted as \textbf{Test Case 1} (easier) and \textbf{Test Case 2} (harder), by varying the number of flow structures and obstacles (and, in our setting, the density of \emph{dynamic} obstacles). In each test suite, we evaluate every method on \textbf{500} randomly generated environments with a fixed start and goal placed at two opposite corners of the map to ensure comparable path length budgets across runs.

\paragraph{Environment instantiation.}
Each episode is generated by sampling: (i) flow sources (e.g., vortices) with random locations, orientations, and strengths; (ii) static obstacles with random positions and sizes; and (iii) dynamic obstacles whose initial states and motion patterns are sampled from the distribution described in Sec.~\ref{sec:env}.%
\footnote{Replace with your exact dynamic obstacle model: e.g., constant-velocity discs, waypoint-following boats, or random-walk agents; also specify speed range, spawn area, and collision radius.}
A square boundary is enforced in each environment; leaving the valid region is counted as an \emph{out-of-bounds} failure, consistent with the evaluation convention used in the reference study.

\paragraph{Control and rollout settings.}
The control time step is set to $\Delta t=\xxx$\,s (we use $\Delta t=0.5$\,s in evaluation to better resolve close interactions near obstacles).
An episode terminates when the agent reaches the goal, collides with an obstacle, runs out of the maximum step budget, or violates the boundary constraint.

\paragraph{Metrics.}
We report four metrics following common practice in local navigation benchmarks:
\begin{itemize}
    \item \textbf{Success rate:} fraction of episodes that reach the goal without collision or out-of-bounds.
    \item \textbf{Out-of-bounds rate:} fraction of episodes ending due to boundary violation.
    \item \textbf{Average time:} mean time-to-goal computed \emph{only} over successful episodes.
    \item \textbf{Average energy:} an action-effort proxy defined as the sum of the magnitudes of control commands along the trajectory, computed over successful episodes.
\end{itemize}

% -----------------------------
% (Moved from: Risk-Sensitive IQN Variants)
% -----------------------------
\paragraph{Risk-sensitive IQN variants.}
We evaluate our Distributional-Risk Planner instantiated with \textbf{Implicit Quantile Networks (IQN)} under both \textbf{fixed} and \textbf{adaptive} risk sensitivity.
Concretely, we adopt a CVaR-style distortion with threshold $\phi \in (0,1]$ and test fixed settings $\phi \in \{0.25,0.50,0.75,1.0\}$ to span a spectrum from strongly tail-focused (conservative) to risk-neutral (greedy).

In addition, we implement a \textbf{distance-adaptive} threshold that increases risk sensitivity as the agent approaches obstacles:
\begin{equation}
\phi =
\begin{cases}
\min(d(\mathbf{X},\mathbf{X}_O))/d_0, & \text{if } \min(d(\mathbf{X},\mathbf{X}_O)) \le d_0,\\
1.0, & \text{otherwise,}
\end{cases}
\label{eq:adaptive_phi}
\end{equation}
where $\mathbf{X}$ is the agent position, $\mathbf{X}_O$ denotes the set of obstacle positions (including dynamic obstacles), and $d_0$ is the sensing range. This design yields near-greedy behavior in free space while automatically emphasizing tail-risk when the environment becomes cluttered.

% -----------------------------
% (Moved from: Baselines)
% -----------------------------
\paragraph{Baselines.}
We compare against both \textbf{classical reactive planners} and \textbf{value-based DRL} baselines.

\paragraph{Classical planners.}
We include (i) \textbf{Artificial Potential Fields (APF)}, which combines an attractive potential toward the goal with a repulsive term induced by obstacle proximity, and (ii) a \textbf{Bug Algorithm (BA)} variant in the spirit of VisBug, which alternates between goal-seeking and boundary-following when an obstacle blocks the direct path.
For APF, LiDAR returns are treated as obstacle points and their repulsive contributions are aggregated; the resulting force direction and magnitude are then mapped to discrete control commands to match the action set. For BA, we approximate the local obstacle tangent from LiDAR reflections and select steering actions that maintain a safe standoff distance while progressing toward the goal when feasible.

\paragraph{Deep RL baselines (expected return).}
Beyond the standard \textbf{DQN} baseline used in prior work,
we additionally evaluate:
\begin{itemize}
    \item \textbf{DDQN} to reduce Q-value overestimation via decoupled action selection and evaluation;
    \item \textbf{D3QN} (Dueling Double DQN) to improve state-value / advantage decomposition and stability.
\end{itemize}
All DRL baselines share the same observation encoding and action space as our method, and are trained under the identical curriculum schedule, replay buffer configuration, and exploration strategy whenever applicable, to ensure a controlled comparison.

% =====================================================================
\subsection{Results}
\label{sec:results}

\paragraph{Overall performance.}
Table~\ref{tab:main_results} summarizes evaluation results on the two test suites. Consistent with the motivation for distributional RL in uncertain navigation, the IQN-based planners achieve higher success rates than expected-return methods, particularly in the harder setting with stronger disturbances and denser obstacles. This advantage is most pronounced when dynamic obstacles create occasional high-cost outcomes (e.g., late collisions) that are under-emphasized by mean-return objectives.

\paragraph{Effect of risk sensitivity.}
Among fixed thresholds, very small $\phi$ (e.g., $\phi=0.25$) can become overly conservative: the agent may prefer large detours to avoid any perceived tail risk, which increases boundary violations (out-of-bounds) and reduces success rate, despite having competitive time/energy on the subset of successful episodes. This mirrors the trade-off observed in the reference evaluation.
Moderate thresholds (e.g., $\phi=0.50$ or $\phi=0.75$) typically offer a better balance between safety and efficiency.

\paragraph{Adaptive risk improves robustness.}
The adaptive policy in Eq.~\eqref{eq:adaptive_phi} attains the strongest overall robustness across test cases: it behaves greedily in open water (maintaining efficiency) while automatically shifting attention to the lower tail of the return distribution when obstacles enter the sensing range. As a result, it preserves success rate under increased difficulty without a significant increase in average time or energy.
Qualitatively, when a narrow passage or interaction with moving obstacles introduces multi-modal outcomes, the adaptive strategy tends to select actions with better worst-case quantiles even if their mean return is similar, which reduces late-stage collisions.

\paragraph{Classical planners under flow disturbances.}
Reactive baselines such as APF and BA can be strongly affected by unknown flows: local repulsive/attractive rules often produce zig-zag or oscillatory trajectories, which inflates traversal time and control effort, especially in the harder setting.
This effect is amplified when dynamic obstacles require quick, coordinated adjustments: purely reactive heuristics may commit to locally myopic maneuvers and lose progress toward the goal.

\paragraph{Expected-return DRL baselines.}
Compared to DQN, DDQN and D3QN generally improve training stability and reduce overestimation-induced failures; however, they still optimize a mean-return criterion and therefore remain less sensitive to rare but severe outcomes. In our navigation task, these rare events correspond to tail-risk interactions (e.g., being swept into obstacles by transient currents or last-moment collisions with moving obstacles), where distributional modeling yields more consistent safety improvements.

\paragraph{Runtime.}
We also report inference-time cost per action decision in Table~\ref{tab:runtime}. Following the reference setup, classical planners are typically fastest due to their lightweight computations, while neural planners introduce modest overhead from a forward pass.
Importantly, all DRL-based methods remain within real-time limits at the control frequency used in our simulator.

% -----------------------------
% Tables (placeholders)
% -----------------------------
\begin{table}[t]
\centering
\caption{Main evaluation results on Test Case 1 and Test Case 2. Report success rate, out-of-bounds rate, average time (successful episodes), and average energy (successful episodes).}
\label{tab:main_results}
\begin{tabular}{lcccc}
\hline
Agent & Success $\uparrow$ & OOB $\downarrow$ & Time(s) $\downarrow$ & Energy $\downarrow$ \\
\hline
IQN (adaptive) & \xxx & \xxx & \xxx & \xxx \\
IQN ($\phi=0.25$) & \xxx & \xxx & \xxx & \xxx \\
IQN ($\phi=0.50$) & \xxx & \xxx & \xxx & \xxx \\
IQN ($\phi=0.75$) & \xxx & \xxx & \xxx & \xxx \\
IQN ($\phi=1.0$)  & \xxx & \xxx & \xxx & \xxx \\
DQN  & \xxx & \xxx & \xxx & \xxx \\
DDQN & \xxx & \xxx & \xxx & \xxx \\
D3QN & \xxx & \xxx & \xxx & \xxx \\
APF  & \xxx & \xxx & \xxx & \xxx \\
BA   & \xxx & \xxx & \xxx & \xxx \\
\hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Runtime per action decision (mean/max).}
\label{tab:runtime}
\begin{tabular}{lcc}
\hline
Agent & Mean (ms) $\downarrow$ & Max (ms) $\downarrow$ \\
\hline
IQN (adaptive) & \xxx & \xxx \\
IQN ($\phi=1.0$) & \xxx & \xxx \\
DQN  & \xxx & \xxx \\
DDQN & \xxx & \xxx \\
D3QN & \xxx & \xxx \\
APF  & \xxx & \xxx \\
BA   & \xxx & \xxx \\
\hline
\end{tabular}
\end{table}