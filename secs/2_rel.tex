\section{Related Works}
Classical reactive planners, learning-based navigation methods, and risk-sensitive Distributional RL have been widely studied for mobile robot and marine vehicle navigation. In this work, we follow the same broad categorization as prior literature, while emphasizing the added difficulty brought by dynamic obstacles interacting with unknown, spatially varying current fields.

Artificial Potential Fields (APF) perform navigation by combining an attractive component toward the goal with repulsive components around obstacles~\cite{ref13}. Although simple and efficient, conventional APF can suffer from local minima and oscillations, especially in cluttered scenes or when the goal lies close to obstacles, motivating many variants that modify the potential design or introduce auxiliary mechanisms to escape undesired equilibria~\cite{ref14,ref15,ref16,ref17,ref18}. Extensions that incorporate relative motion information or evolutionary optimization have also been explored for dynamic environments~\cite{ref19,ref20,ref21}, but these reactive potential designs can still be sensitive when currents and moving obstacles jointly create rapidly changing, nonstationary risk regions.

Bug-type Algorithms (BA) are another class of reactive planners dating back to early work in the 1980s~\cite{ref22}. Representative variants such as Bug1/Bug2 alternate between goal-seeking motion and boundary-following upon obstacle encounter~\cite{ref23}, and have been applied across platforms including wheeled robots, aerial robots, and USVs~\cite{ref24,ref25,ref12}. Later variants improve practicality by reducing repeated detours and exploiting sensing to find shorter paths~\cite{ref26,ref27,ref28,ref29,ref30}. However, their performance can degrade under strong disturbances (e.g., currents) and in the presence of dynamic obstacles that trigger frequent mode switching or oscillatory interactions.

Deep Reinforcement Learning (DRL) based planners have shown strong ability to navigate in unknown environments by learning policies from interaction data. Among value-based DRL methods, Deep Q-Network (DQN) learns an action-value function with neural approximation, but may suffer from overestimation bias and unstable learning in stochastic settings; Double DQN (DDQN) mitigates this issue by decoupling action selection and evaluation in the target computation, typically improving stability and robustness. Building on these ideas, the dueling architecture further decomposes $Q(s,a)$ into state value and advantage, and when combined with DDQN and prioritized replay (often referred to in practice as D3QN), it can yield better sample efficiency and more reliable learning in navigation/control tasks. Empirically, DRL has achieved collision-free navigation in unseen indoor environments and robustness in rough-terrain scenarios~\cite{ref31,ref32,ref33}; for marine platforms, RL has been investigated for underactuated USV control under uncertain dynamics~\cite{ref34} and for avoiding moving obstacles such as boats~\cite{ref35}. Nevertheless, these DQN-family methods still optimize (or approximate) \emph{expected} return, which may be misaligned with safety requirements when failures are rare but high-impact, as is common in current-disturbed environments with dynamic obstacles. More recently, Distributional RL methods explicitly model the \emph{return distribution} rather than only its expectation, providing a richer representation of uncertainty and a natural handle for risk-aware decision making~\cite{ref7,ref8}. In particular, Implicit Quantile Networks (IQN) represent the value distribution via quantile regression by sampling quantile levels and predicting the corresponding returns, enabling the policy to reason about tail outcomes and to adjust risk sensitivity through how these quantiles are aggregated during action selection. Recent studies suggest that distributional approaches can improve safety relative to expectation-based DRL and can provide adjustable sensitivity to collision risks for navigation tasks~\cite{ref36,ref37}; related work further considers automatically adapting risk sensitivity to match environmental difficulty~\cite{ref38}. Building on this line, Distributional-Risk Planner adopts IQN-style distributional value learning for USV navigation in unknown current fields and extends the setting to include dynamic obstacles, pairing distributional return modeling with adaptive risk modulation to improve safety and reliability.