\section{Problem Formulation}
We model the USV navigation task as a Markov Decision Process (MDP) defined by the tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where $\mathcal{S}$ and $\mathcal{A}$ denote the state space and action space, respectively. At each discrete time step $t$, the agent observes the current state $s_t \in \mathcal{S}$ and chooses an action $a_t \in \mathcal{A}$. The system then evolves to the next state according to the transition dynamics $s_{t+1} \sim P(\cdot \mid s_t, a_t)$, after which the agent receives a scalar reward $r_{t+1} = R(s_t, a_t)$ and proceeds with the subsequent decision.

A standard objective in reinforcement learning is to learn a policy $\pi$ that maximizes the expected discounted cumulative return. This objective is captured by the action-value function (or $Q$-function) under policy $\pi$, defined as
\begin{equation}
Q^{\pi}(s,a) = \mathbb{E}_{\pi}\!\left[\sum_{k=0}^{\infty}\gamma^{k}\, r_{t+k+1}\,\Big|\, s_t=s,\, a_t=a \right],
\label{eq:q_def}
\end{equation}
where $\gamma \in [0,1)$ is the discount factor controlling the relative importance of future rewards. Starting from a given state--action pair $(s_t,a_t)$, the expectation is taken over future transitions and actions when following $\pi$ thereafter.

The $Q$-function satisfies the Bellman expectation equation
\begin{equation}
Q^{\pi}(s,a) = \mathbb{E}\!\left[ R(s,a) \right] + \gamma\, \mathbb{E}\!\left[ Q^{\pi}(s',a') \right],
\label{eq:bellman_exp}
\end{equation}
where $s' \sim P(\cdot \mid s,a)$ and $a' \sim \pi(\cdot \mid s')$. In particular, an optimal policy can be obtained by applying the Bellman optimality operator, which is commonly written as
\begin{equation}
\mathcal{T}Q(s,a) := \mathbb{E}\!\left[ R(s,a) \right] + \gamma\, \mathbb{E}\!\left[\max_{a'} Q(s',a') \right],
\label{eq:bellman_opt}
\end{equation}
with $s' \sim P(\cdot \mid s,a)$. The fixed point of this operator corresponds to the optimal action-value function, from which a greedy policy can be derived.